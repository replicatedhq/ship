apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-0.5.1
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch
---
apiVersion: v1
data:
  elasticsearch.yml: |-
    cluster.name: elasticsearch

    node.data: ${NODE_DATA:true}
    node.master: ${NODE_MASTER:true}
    node.ingest: ${NODE_INGEST:true}
    node.name: ${HOSTNAME}

    network.host: 0.0.0.0
    # see https://github.com/kubernetes/kubernetes/issues/3595
    bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}

    discovery:
      zen:
        ping.unicast.hosts: ${DISCOVERY_SERVICE:}
        minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}

    # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679
    processors: ${PROCESSORS:}

    # avoid split-brain w/ a minimum consensus of two masters plus a data node
    gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}
    gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}
    gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}
    gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}
    gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}

    # Extra Configuration

    # X-Pack

    # Search Guard
  log4j2.properties: |-
    status = error

    appender.console.type = Console
    appender.console.name = console
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n

    rootLogger.level = info
    rootLogger.appenderRef.console.ref = console
  pre-stop-hook.sh: "#!/bin/bash\nset -e\n\nSERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount\nKUBE_TOKEN=$(<${SERVICE_ACCOUNT_PATH}/token)\nKUBE_NAMESPACE=$(<${SERVICE_ACCOUNT_PATH}/namespace)\n\nSTATEFULSET_NAME=$(echo
    \"${HOSTNAME}\" | sed 's/-[0-9]*$//g')\nINSTANCE_ID=$(echo \"${HOSTNAME}\" | grep
    -o '[0-9]*$')\n\necho \"Prepare stopping of Pet ${KUBE_NAMESPACE}/${HOSTNAME}
    of StatefulSet ${KUBE_NAMESPACE}/${STATEFULSET_NAME} instance_id ${INSTANCE_ID}\"\n\nexport
    STATEFULSET_STATUS=$(\n  curl -s \\\n    --cacert ${SERVICE_ACCOUNT_PATH}/ca.crt
    \\\n    -H \"Authorization: Bearer $KUBE_TOKEN\" \\\n    \"https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_PORT_443_TCP_PORT}/apis/apps/v1beta1/namespaces/${KUBE_NAMESPACE}/statefulsets/${STATEFULSET_NAME}/status\"\n)\nINSTANCES_DESIRED=$(\n\tpython
    - <<-EOF\n\t\timport json\n\t\timport os\n\n\t\tobj = json.loads(os.environ.get('STATEFULSET_STATUS'))\n\t\tprint(obj['spec']['replicas'])\n\tEOF\n)\n\necho
    \"Desired instance count is ${INSTANCES_DESIRED}\"\n\nif [ \"${INSTANCE_ID}\"
    -lt \"${INSTANCES_DESIRED}\" ]; then\n  echo \"No data migration needed\"\n  exit
    0\nfi\n\necho \"Prepare to migrate data of the node\"\n\nexport NODE_STATS=$(\n
    \ curl -X GET -s \\\n    http://127.0.0.1:9200/_nodes/stats\n)\nNODE_IP=$(\n\tpython
    - <<-EOF\n\t\timport json\n\t\timport os\n\n\t\tobj = json.loads(os.environ.get('NODE_STATS'))\n\t\tkey
    = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'),
    obj['nodes'].keys()))[0]\n\t\tnode = obj['nodes'][key]\n\t\tprint(node['host'])\n\tEOF\n)\n\necho
    \"Move all data from node ${NODE_IP}\"\n\ncurl -X PUT -H \"Content-Type: application/json\"
    -s \\\n  http://127.0.0.1:9200/_cluster/settings \\\n  --data \"{\n      \\\"transient\\\"
    :{\n          \\\"cluster.routing.allocation.exclude._ip\\\" : \\\"${NODE_IP}\\\"\n
    \     }\n    }\"\necho\n\necho \"Wait for node documents to become empty\"\nDOC_COUNT=$(\n\tpython
    - <<-EOF\n\t\timport json\n\t\timport os\n\n\t\tobj = json.loads(os.environ.get('NODE_STATS'))\n\t\tkey
    = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'),
    obj['nodes'].keys()))[0]\n\t\tnode = obj['nodes'][key]\n\t\tprint(node['indices']['docs']['count'])\n\tEOF\n)\n\nwhile
    [ \"${DOC_COUNT}\" -gt 0 ]; do\n  export NODE_STATS=$(\n    curl -X GET -s \\\n
    \     http://127.0.0.1:9200/_nodes/stats\n  )\n  DOC_COUNT=$(\n\t\tpython - <<-EOF\n\t\t\timport
    json\n\t\t\timport os\n\n\t\t\tobj = json.loads(os.environ.get('NODE_STATS'))\n\t\t\tkey
    = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'),
    obj['nodes'].keys()))[0]\n\t\t\tnode = obj['nodes'][key]\n\t\t\tcount = node['indices']['docs']['count']\n\t\t\tprint(count)\n\t\tEOF\n
    \ )\n  echo \"Node contains ${DOC_COUNT} documents\"\n  sleep 1\ndone\n\necho
    \"Wait for node shards to become empty\"\nexport SHARD_STATS=$(\n  curl -X GET
    -s \\\n    http://127.0.0.1:9200/_cat/shards?format=json\n)\nSHARD_COUNT=$(\n\tpython
    - <<-EOF\n\t\timport json\n\t\timport os\n\n\t\tobj = json.loads(os.environ.get('SHARD_STATS'))\n\t\tcount
    = len(filter(lambda datum: datum['node'] == os.environ.get('HOSTNAME'), obj))\n\t\tprint(count)\n\tEOF\n)\nwhile
    [ \"${SHARD_COUNT}\" -gt 0 ]; do\n  export SHARD_STATS=$(\n    curl -X GET -s
    \\\n      http://127.0.0.1:9200/_cat/shards?format=json\n  )\n  SHARD_COUNT=$(\n\t\tpython
    - <<-EOF\n\t\t\timport json\n\t\t\timport os\n\n\t\t\tobj = json.loads(os.environ.get('SHARD_STATS'))\n\t\t\tcount
    = len(filter(lambda datum: datum['node'] == os.environ.get('HOSTNAME'), obj))\n\t\t\tprint(count)\n\t\tEOF\n
    \ )\n  echo \"Node contains ${SHARD_COUNT} shards\"\n  sleep 1\ndone\n\necho \"Node
    clear to shutdown\""
kind: ConfigMap
metadata:
  labels:
    app: elastic-stack-elasticsearch
    chart: elasticsearch-1.16.0
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.16.0
    component: client
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-client
spec:
  ports:
  - port: 9200
    targetPort: http
  selector:
    app: elasticsearch
    component: client
    release: elastic-stack
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-0.5.1
    component: master
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-master
spec:
  clusterIP: None
  ports:
  - port: 9300
    targetPort: 9300
  selector:
    app: elasticsearch
    component: master
    release: elastic-stack
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kibana
    chart: kibana-1.1.2
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-kibana
spec:
  ports:
  - name: kibana
    port: 80
    protocol: TCP
    targetPort: 5601
  selector:
    app: kibana
    release: elastic-stack
  type: ClusterIP
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.16.0
    component: client
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-client
spec:
  replicas: 2
  template:
    metadata:
      annotations:
        checksum/config: 4f07b9e19327171c37a9c353906c75a1f454cd31c3dfc600a8882d6e36713c49
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      labels:
        app: elasticsearch
        component: client
        release: elastic-stack
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: elasticsearch
                  component: client
                  release: elastic-stack
              topologyKey: kubernetes.io/hostname
            weight: 1
      containers:
      - env:
        - name: DISCOVERY_SERVICE
          value: elastic-stack-elasticsearch-master.default.svc.cluster.local
        - name: NODE_DATA
          value: "false"
        - name: NODE_INGEST
          value: "false"
        - name: ES_HEAP_SIZE
          value: 512m
        - name: NODE_MASTER
          value: "false"
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_JAVA_OPTS
          value: -Djava.net.preferIPv4Stack=true
        - name: MINIMUM_MASTER_NODES
          value: "2"
        image: gcr.io/cos-containers/elasticsearch:5.4.2-xpack
        imagePullPolicy: Always
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - curl --request GET --silent --output /dev/null http://127.0.0.1:9200/_cluster/health?wait_for_status=yellow
          initialDelaySeconds: 90
        name: elasticsearch
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - curl --request GET --silent --output /dev/null http://127.0.0.1:9200/_cluster/health?wait_for_status=yellow
          initialDelaySeconds: 5
        resources:
          limits:
            cpu: "1"
          requests:
            cpu: 25m
            memory: 512Mi
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          readOnly: true
          subPath: elasticsearch.yml
        - mountPath: /usr/share/elasticsearch/config/log4j2.properties
          name: config
          readOnly: true
          subPath: log4j2.properties
      initContainers:
      - command:
        - sh
        - -c
        - |-
          # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-prod-mode
          sysctl -w vm.max_map_count=262144
          # To increase the ulimit
          # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults
          ulimit -l unlimited
        image: busybox
        name: increase-memory-limits
        securityContext:
          privileged: true
      securityContext:
        fsGroup: 1000
      serviceAccountName: elastic-stack-elasticsearch
      volumes:
      - configMap:
          name: elastic-stack-elasticsearch
        name: config
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: kibana
    chart: kibana-1.1.2
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-kibana
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kibana
        release: elastic-stack
    spec:
      containers:
      - env:
        - name: XPACK_MONITORING_ENABLED
          value: "true"
        - name: ELASTICSEARCH_URL
          value: http://elastic-stack-elasticsearch:9200
        image: docker.elastic.co/kibana/kibana:5.4.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /
            port: 5601
          initialDelaySeconds: 180
        name: kibana
        ports:
        - containerPort: 5601
          name: http
        readinessProbe:
          httpGet:
            path: /status
            port: 5601
          initialDelaySeconds: 180
          periodSeconds: 10
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.16.0
    component: data
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-data
spec:
  replicas: 2
  serviceName: elastic-stack-elasticsearch-data
  template:
    metadata:
      annotations:
        checksum/config: 4f07b9e19327171c37a9c353906c75a1f454cd31c3dfc600a8882d6e36713c49
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      labels:
        app: elasticsearch
        component: data
        release: elastic-stack
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: elasticsearch
                  component: data
                  release: elastic-stack
              topologyKey: kubernetes.io/hostname
            weight: 1
      containers:
      - env:
        - name: DISCOVERY_SERVICE
          value: elastic-stack-elasticsearch-master.default.svc.cluster.local
        - name: NODE_MASTER
          value: "false"
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_HEAP_SIZE
          value: 1536m
        - name: ES_JAVA_OPTS
          value: -Djava.net.preferIPv4Stack=true
        - name: MINIMUM_MASTER_NODES
          value: "2"
        image: gcr.io/cos-containers/elasticsearch:5.4.2-xpack
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/bash
              - /pre-stop-hook.sh
        name: elasticsearch
        ports:
        - containerPort: 9300
          name: transport
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - curl --request GET --silent --output /dev/null http://127.0.0.1:9200/_cluster/health?local=true
          initialDelaySeconds: 5
        resources:
          limits:
            cpu: "1"
          requests:
            cpu: 25m
            memory: 1536Mi
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          readOnly: true
          subPath: elasticsearch.yml
        - mountPath: /usr/share/elasticsearch/config/log4j2.properties
          name: config
          readOnly: true
          subPath: log4j2.properties
        - mountPath: /pre-stop-hook.sh
          name: config
          subPath: pre-stop-hook.sh
      initContainers:
      - command:
        - sh
        - -c
        - |-
          # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-prod-mode
          sysctl -w vm.max_map_count=262144
          # To increase the ulimit
          # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults
          ulimit -l unlimited
        image: busybox
        name: increase-memory-limits
        securityContext:
          privileged: true
      securityContext:
        fsGroup: 1000
      serviceAccountName: elastic-stack-elasticsearch
      terminationGracePeriodSeconds: 3600
      volumes:
      - configMap:
          name: elastic-stack-elasticsearch
        name: config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 4Gi
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.16.0
    component: master
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-master
spec:
  replicas: 2
  serviceName: elastic-stack-elasticsearch-master
  template:
    metadata:
      annotations:
        checksum/config: 4f07b9e19327171c37a9c353906c75a1f454cd31c3dfc600a8882d6e36713c49
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      labels:
        app: elasticsearch
        component: master
        release: elastic-stack
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: elasticsearch
                  component: master
                  release: elastic-stack
              topologyKey: kubernetes.io/hostname
            weight: 1
      containers:
      - env:
        - name: DISCOVERY_SERVICE
          value: elastic-stack-elasticsearch-master.default.svc.cluster.local
        - name: NODE_DATA
          value: "false"
        - name: NODE_INGEST
          value: "false"
        - name: ES_HEAP_SIZE
          value: 512m
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_JAVA_OPTS
          value: -Djava.net.preferIPv4Stack=true
        - name: MINIMUM_MASTER_NODES
          value: "2"
        image: gcr.io/cos-containers/elasticsearch:5.4.2-xpack
        imagePullPolicy: Always
        name: elasticsearch
        ports:
        - containerPort: 9300
          name: transport
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - curl --request GET --silent --output /dev/null http://127.0.0.1:9200/_cluster/health?local=true
          initialDelaySeconds: 5
        resources:
          limits:
            cpu: "1"
          requests:
            cpu: 25m
            memory: 512Mi
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          readOnly: true
          subPath: elasticsearch.yml
        - mountPath: /usr/share/elasticsearch/config/log4j2.properties
          name: config
          readOnly: true
          subPath: log4j2.properties
      initContainers:
      - command:
        - sh
        - -c
        - |-
          # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
          # and https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-prod-mode
          sysctl -w vm.max_map_count=262144
          # To increase the ulimit
          # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults
          ulimit -l unlimited
        image: busybox
        name: increase-memory-limits
        securityContext:
          privileged: true
      securityContext:
        fsGroup: 1000
      serviceAccountName: elastic-stack-elasticsearch
      volumes:
      - configMap:
          name: elastic-stack-elasticsearch
        name: config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 4Gi
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-0.5.1
    component: client
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-client
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: elasticsearch
      component: client
      release: elastic-stack
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-0.5.1
    component: data
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-data
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: elasticsearch
      component: data
      release: elastic-stack
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-0.5.1
    component: master
    heritage: Tiller
    release: elastic-stack
  name: elastic-stack-elasticsearch-master
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: elasticsearch
      component: master
      release: elastic-stack
